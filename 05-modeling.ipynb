{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        output = self.fc(lstm_out[-1])\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Linear Classifier model\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)  # 입력 차원, 출력 차원\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))  # 시그모이드 활성화 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define custom Dataset for train and test data\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # x = self.data[index]\n",
    "        # y = self.labels.iloc[index]\n",
    "        x = torch.tensor(self.data[index]) # 데이터를 Tensor로 변환\n",
    "        y = self.labels.iloc[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Vector - Simple Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train: 160000\n",
      "size of test: 40000\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the entire dataset from a DataFrame\n",
    "df = pd.read_csv('/home/kyuyeon/문서/kmu/23-1/bigdata-latest/movie/data/ratings-mec.csv') # replace with your own code to load the data from a CSV file\n",
    "sentences = df['tokens'].apply(ast.literal_eval) # assuming 'tokens' is the column containing tokenized sentences, modify this according to your DataFrame\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(sentences, df['label'], test_size=0.2, random_state=42) # modify this line to include labels\n",
    "# train_sentences, valid_sentences, train_labels, valid_labels = train_test_split(train_sentences, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f'size of train: {len(train_sentences)}')\n",
    "# print(f'size of valid: {len(valid_sentences)}')\n",
    "print(f'size of test: {len(test_sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized sentences to word embeddings vectors\n",
    "def sentence2vec(model, sentences):\n",
    "    vectors = []\n",
    "    for sentence in sentences:\n",
    "        word_vectors = []\n",
    "        for word in sentence:\n",
    "            if word in model.wv.key_to_index:\n",
    "                word_vectors.append(model.wv[word])\n",
    "        if word_vectors:\n",
    "            word_vectors = np.array(word_vectors)\n",
    "            sentence_vector = np.mean(word_vectors, axis=0) # 단어 벡터들을 평균내어 문장 벡터 생성\n",
    "        else:\n",
    "            sentence_vector = np.array([0]*100)\n",
    "        vectors.append(sentence_vector)\n",
    "    vectors = np.array(vectors)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "1. Word2Vec (CBOW)\n",
    "2. Word2Vec (Skip-gram)\n",
    "3. FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000, 100)\n",
      "(160000,)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train Word2Vec model on the tokenized sentences\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "\n",
    "train_vectors = sentence2vec(word2vec_model, train_sentences)\n",
    "test_vectors = sentence2vec(word2vec_model, test_sentences)\n",
    "print(train_vectors.shape) # (160000, 100)\n",
    "print(train_labels.shape) # (160000,)\n",
    "\n",
    "# Create DataLoader for train and test data\n",
    "train_dataset = TextDataset(train_vectors, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=False)\n",
    "\n",
    "test_dataset = TextDataset(test_vectors, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000, 100)\n",
      "(160000,)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Train FastText model on the tokenized sentences\n",
    "fasttext_model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "fasttext_model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "\n",
    "train_vectors = sentence2vec(fasttext_model, train_sentences)\n",
    "test_vectors = sentence2vec(fasttext_model, test_sentences)\n",
    "print(train_vectors.shape) # (160000, 100)\n",
    "print(train_labels.shape) # (160000,)\n",
    "\n",
    "# Create DataLoader for train and test data\n",
    "train_dataset = TextDataset(train_vectors, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=False)\n",
    "\n",
    "test_dataset = TextDataset(test_vectors, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the linear classifier model\n",
    "input_size = train_vectors.shape[1]\n",
    "output_size = 1 # assuming binary classification (1 or 0)\n",
    "model = LinearClassifier(input_size, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in tqdm(range(10)):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "        x = x.float()  # 입력 데이터를 float 형태로 변환\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x)\n",
    "        loss = criterion(predictions, y.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch, 10000, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in test_loader:\n",
    "        x, y = batch\n",
    "        x = x.float()  # 입력 데이터를 float 형태로 변환\n",
    "        predictions = model(x)\n",
    "        predicted_labels = (predictions > 0.5).float()\n",
    "        total += y.size(0)\n",
    "        correct += (predicted_labels == y.unsqueeze(1).float()).sum().item()\n",
    "    accuracy = (correct / total) * 100\n",
    "    print('Test Accuracy: {:.2f}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5847/294808729.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_5847/294808729.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch Loss: 0.6412\n",
      "Epoch [2/10], Batch Loss: 0.5951\n",
      "Epoch [3/10], Batch Loss: 0.6103\n",
      "Epoch [4/10], Batch Loss: 0.4543\n",
      "Epoch [5/10], Batch Loss: 0.7007\n",
      "Epoch [6/10], Batch Loss: 0.5084\n",
      "Epoch [7/10], Batch Loss: 0.4986\n",
      "Epoch [8/10], Batch Loss: 0.5381\n",
      "Epoch [9/10], Batch Loss: 0.5535\n",
      "Epoch [10/10], Batch Loss: 0.7661\n"
     ]
    }
   ],
   "source": [
    "input_size = 1 # word embeddings의 크기\n",
    "hidden_size = 128 # LSTM의 hidden unit 개수\n",
    "output_size = 1 # 출력 유닛 개수 (이진 분류)\n",
    "\n",
    "lstm_model = LSTMModel(input_size, hidden_size, output_size) # LSTM 모델 초기화\n",
    "criterion = nn.BCELoss() # BCE 손실 함수\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001) # Adam 옵티마이저\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # GPU 사용 가능 여부에 따라 디바이스 설정\n",
    "lstm_model.to(device) # 모델을 GPU로 이동\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        # 데이터를 텐서로 변환하고 GPU로 이동\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad() # 그래디언트 초기화\n",
    "        inputs = inputs.view(32, 100, -1)\n",
    "        inputs = inputs.permute(1, 0, 2)\n",
    "        # print(inputs.shape) # torch.Size([100, 32, 1])\n",
    "        outputs = lstm_model(inputs) # LSTM 모델에 입력 전달\n",
    "        loss = criterion(outputs.squeeze(), labels.squeeze()) # 손실 계산\n",
    "        loss.backward() # 역전파\n",
    "        optimizer.step() # 가중치 업데이트\n",
    "\n",
    "    print('Epoch [{}/{}], Batch Loss: {:.4f}'.format(epoch + 1, 10, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5847/594539514.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_5847/594539514.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5258, Accuracy: 72.67%\n"
     ]
    }
   ],
   "source": [
    "lstm_model.eval() # 모델을 평가 모드로 변경\n",
    "test_loss = 0 # 테스트 손실 초기화\n",
    "correct = 0 # 정확한 예측 개수 초기화\n",
    "\n",
    "with torch.no_grad(): # 그래디언트 계산 비활성화\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "        inputs = inputs.view(32, 100, -1)\n",
    "        inputs = inputs.permute(1, 0, 2)\n",
    "        outputs = lstm_model(inputs) # LSTM 모델에 입력 전달\n",
    "        test_loss += criterion(outputs.squeeze(), labels.squeeze()).item() # 테스트 손실 누적 계산\n",
    "\n",
    "        predicted = torch.round(outputs.squeeze()) # 예측값을 0 또는 1로 변환\n",
    "        correct += (predicted == labels.squeeze()).sum().item() # 정확한 예측 개수 누적 계산\n",
    "\n",
    "test_loss /= len(test_loader) # 배치 수로 나누어 평균 테스트 손실 계산\n",
    "accuracy = correct / (len(test_loader) * 32) # 전체 예측 개수로 나누어 정확도 계산\n",
    "\n",
    "print('Test Loss: {:.4f}, Accuracy: {:.2%}'.format(test_loss, accuracy))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
